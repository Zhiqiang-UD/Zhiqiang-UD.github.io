<!DOCTYPE html>
<html lang="en">
<head>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <title>
        
            Zhiqiang
        
    </title>
    <link rel="icon" href="/img/favicon.png"/>
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="/css/font-awesome.min.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <link rel="stylesheet" href="/css/hljs.min.css">
    <script src="/js/hljs.min.js"></script>  
    <script src="/js/gitment.browser.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->  
</head>
<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header" id="header">
    <h1>
        <a class="title" href="/">
            Zhiqiang
        </a>
    </h1>
    <h2>
        <a class="motto">
            Be Smart and Work Hard!
        </a>
    </h2>
    <nav class="navbar">
        <ul class="menu">
            
            
                <li class="menu-item">
                    <a href="/" class="menu-item-link">
                        Home
                    </a>
                </li>
            
                
            
                <li class="menu-item">
                    <a href="/archives/" class="menu-item-link">
                        Archives
                    </a>
                </li>
            
                
            
                <li class="menu-item">
                    <a href="/about/" class="menu-item-link">
                        About
                    </a>
                </li>
            
                
            
                <li class="menu-item">
                    <a href="https://github.com/Zhiqiang-UD" class="menu-item-link">
                        Github
                    </a>
                </li>
            
                
            
                <li class="menu-item">
                    <a href="/atom.xml" class="menu-item-link">
                        RSS
                    </a>
                </li>
            
                
            
                
                
                <li class="menu-item">
                    <a class="menu-item-link search">
                        Search                   
                        <i class="fa fa-long-arrow-right search-icon" aria-hidden="true"></i>
                    </a>
                        <input placeholder="Search..." class="search-input" style="display:none;border:none!important;" onkeydown="onEnter(event)" onkeypress="onEnter(event)"></input>
                </li>
                
        </ul>
    </nav>
</header>
    <main class="main">
        <section class="posts">
    
        <article class="post">
            <h1>
                <a class="title" href="/2018/12/12/CISC_662_Project_Report/"> 
                    Parallelization and Analysis of Shortest Path Algorithms  
                </a>
            </h1>
                        <div class="meta">
                <a class="date"> 
                    <i class="fa fa-calendar" aria-hidden="true"></i>                    
                    2018-12-12   
                </a>
                
                
                
                    
            </div>
            <div class="content">
                
                <h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A central problem in graph theory is the shortest path problem which is to find the shortest path between two nodes (vertices) in a graph so that the sum of the weight of its constituent edges is minimum. There are four variations of the shortest path problem namely, single-source shortest path (SSSP), breadth-first search (BFS), all-pairs shortest path (APSP), and single-source widest path (SSWP). Many algorithms are in use today which solve this problem. Some of the most popular algorithms are Dijkstra’s algorithm, Bellman-Ford algorithm, A* search algorithm, and Floyd-Warshall algorithm. These algorithms can be applied to undirected as well as directed graphs. Interesting research has been done on this problem due to its many useful real-world applications like road networks, community detection, currency exchange, logistics, electronic design, etc. In this project, we focused on the parallelization of three algorithms - Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm and analyze the speedups against sequential implementations.</p>
<h2 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h2><p>Parallelization of the classical shortest path algorithms can be done in several ways. Some of the ways have been discussed by B. Popa <em>et al.</em> [1] Another approach to parallelize Dijkstra’s algorithm has been discussed by A. Crauser <em>et al.</em> [2]<br>To discuss parallelizing the classical algorithms, we discuss the main idea of the three algorithms we used in our experiment, namely: Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm in the following.<br>Dijkstra’s algorithm finds the shortest path from a single source and at each step, it finds the minimal distant node, tries to find the shortest paths to other nodes using the recently found shortest node. This algorithm does not consider negative weights and thus cannot be used in a graph with negative weights.<br>Bellman-Ford algorithm also finds the shortest path from a single source. However, at each step in contrary to Dijkstra’s algorithm, this algorithm updates each node’s distance from the source by the observing the edges. Bellman-Ford algorithm can detect negative weight cycles in a graph and also can be used in a graph with negative weights.<br>Floyd-Warshall algorithm finds all pair shortest path and does that by observing each node at a time and update paired shortest distances if the node can contribute. This algorithm also can be used in a graph with negative weights.<br>The pseudocodes of Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm are given in Figure 1 below.<br><img src="/img/Pseudocode.png" alt="Figure 1. Pseudocode of Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm"><span class="image_caption">Figure 1. Pseudocode of Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm</span></p>
<p>The running time and space complexity of the algorithms are given in Table 1 below:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">ALGORITHM</th>
<th style="text-align:center">Time Complexity</th>
<th style="text-align:center">Space Complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Dijkstra</td>
<td style="text-align:center">$O(V^2)$</td>
<td style="text-align:center">$O(V^2)$</td>
</tr>
<tr>
<td style="text-align:center">Bellman-Ford</td>
<td style="text-align:center">$O(VE)$</td>
<td style="text-align:center">$O(V^2)$</td>
</tr>
<tr>
<td style="text-align:center">Floyd-Warshall</td>
<td style="text-align:center">$O(V^3)$</td>
<td style="text-align:center">$O(V^3)$</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Our-Solution"><a href="#Our-Solution" class="headerlink" title="Our Solution"></a>Our Solution</h2><p>The three algorithms we chose for our experiment are classic and they have been used in numerous applications. If we observe the time complexity of the algorithms, we see all three are having significant calculation part i.e. arithmetic integrity. According to the roofline model, the more arithmetic intensity an algorithm has, the more performance gain can be achieved using parallelization while increasing the data set. Therefore, we used parallelization to gain performance. For parallelization we used OpenMP.</p>
<p>OpenMP is an API for writing multi-threaded applications. The API supports C/C++ and Fortran on a wide variety of architectures. OpenMP provides a portable, scalable model for developers of shared memory parallel applications.<br>OpenMP is an abbreviation for Open Multi-Processing. It is comprised of three primary API components:</p>
<ul>
<li>Compiler Directives</li>
<li>Runtime Library Routines</li>
<li>Environment Variables</li>
</ul>
<p>OpenMP uses the fork-join model of parallel execution. All OpenMP programs begin as a single process: the master thread. The master thread executes sequentially until the first parallel region construct is encountered. The master thread then creates a team of parallel threads. The statements in the program that are enclosed by the parallel region construct are then executed in parallel among the various team threads. When the team threads complete the statements in the parallel region construct, they synchronize and terminate, leaving only the master thread. The number of parallel regions and the threads that comprise them are arbitrary.</p>
<p>Because OpenMP (Fig. 2) is a shared memory programming model, most data within a parallel region is shared by default. OpenMP provides a way for the programmer to explicitly specify how data is “scoped” if the default shared scoping is not desired.<br><img src="/img/Fig2.png" alt="Figure 2. OpenMP thread Model"><span class="image_caption">Figure 2. OpenMP thread Model</span></p>
<h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><p>For the experimental setup, we used four machines. Three of the machines were personal laptops. The fourth one is a computer node of Farber cluster which is the University of Delaware’s second Community Cluster. Farber cluster uses distributed-memory running on Linux operating system (Cent OS). A summary of these machines can be seen in the following table.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Machine #</th>
<th style="text-align:center">Operating System</th>
<th style="text-align:center">Architecture</th>
<th style="text-align:center"># cores</th>
<th style="text-align:center"># threads</th>
<th style="text-align:center">RAM (GB)</th>
<th style="text-align:center">frequency (GHz)</th>
<th style="text-align:center">Compiler</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1 (laptop)</td>
<td style="text-align:center">OS X</td>
<td style="text-align:center">Intel Core i5 3210M</td>
<td style="text-align:center">2</td>
<td style="text-align:center">4</td>
<td style="text-align:center">8</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">Clang</td>
</tr>
<tr>
<td style="text-align:center">2 (laptop)</td>
<td style="text-align:center">Windows</td>
<td style="text-align:center">Intel Core i7 7500U</td>
<td style="text-align:center">2</td>
<td style="text-align:center">4</td>
<td style="text-align:center">8</td>
<td style="text-align:center">2.7</td>
<td style="text-align:center">OpenMP for Visual Studio 2015</td>
</tr>
<tr>
<td style="text-align:center">3 (laptop)</td>
<td style="text-align:center">Ubuntu</td>
<td style="text-align:center">Intel Core i7 8550U</td>
<td style="text-align:center">4</td>
<td style="text-align:center">8</td>
<td style="text-align:center">16</td>
<td style="text-align:center">1.8</td>
<td style="text-align:center">GCC</td>
</tr>
<tr>
<td style="text-align:center">4 (farber cluster)</td>
<td style="text-align:center">Cent OS</td>
<td style="text-align:center">Intel(R) Xeon(R) E5-2670</td>
<td style="text-align:center">20</td>
<td style="text-align:center">20</td>
<td style="text-align:center">125</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">GCC/ICC(intel)</td>
</tr>
</tbody>
</table>
</div>
<p>To see the compiler effect, we compile the performance using two different compilers: GCC and ICC (intel compiler), both of which are available on Farber. </p>
<p>The data sets used in our project are fake maps generated with script containing different number of nodes. We chose the number node to be 20, 100, 500, 1000 to see the effect of map size. For all the running time reported in this project, it is an average of running the same program on the same machine for 5 times. </p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Serial-Running-Time"><a href="#Serial-Running-Time" class="headerlink" title="Serial Running Time"></a>Serial Running Time</h3><p>The sequential running time is plotted in Fig. 3. This is the running time of the graph with 1000 nodes on machine 3. From the figure we can see that Floyd Warshall algorithm is the slowest one and scales close to $O(V^3)$. As the nodes increases, Dijkstra’s algorithm is more efficient: $O(V^2)$ becomes smaller than $O(VE)$ as the number of edge gets larger.<br><img src="/img/Scale of Running Time.png" alt="Figure 3. Scale of running time with increasing nodes in the graph"><span class="image_caption">Figure 3. Scale of running time with increasing nodes in the graph</span></p>
<h3 id="Effects-of-Compiler-GCC-vs-ICC"><a href="#Effects-of-Compiler-GCC-vs-ICC" class="headerlink" title="Effects of Compiler: GCC vs. ICC"></a>Effects of Compiler: GCC vs. ICC</h3><p>On farber, we compiled all three algorithms with ICC and GCC. The running time on the 1000 node graph is shown in Fig. 4. The ICC significantly outperformed GCC before we go over 8 threads. For Bellman-Ford and Floyd-Warshall algorithms, the difference is as much as one order in magnitude. This means GCC is not optimized as ICC for the intel CPU. Another interesting result is that the speed up with more threads is more prominent with GCC, as their running time become comparable with 16 threads.<br><img src="/img/Farber_threads_GCC_ICC.png" alt="Figure 4. GCC and ICC compiler"><span class="image_caption">Figure 4. GCC and ICC compiler</span></p>
<h3 id="Parallel-Running-Time-on-Machine-1-OS-X"><a href="#Parallel-Running-Time-on-Machine-1-OS-X" class="headerlink" title="Parallel Running Time on Machine 1: OS X"></a>Parallel Running Time on Machine 1: OS X</h3><p>On the macbook pro with OS X, we see decreasing running time from 1 to 2 threads. More speed up is achieved with more nodes with parallelization.<br>For all the three algorithms, there’re no improvement from 2 to 4 threads. This is possibly because it only has 2 physical cores and the 4 logical threads are not so efficient.<br><img src="/img/Mac_threads.png" alt="Figure 5. Parallel Running Time on Machine 1"><span class="image_caption">Figure 5. Parallel Running Time on Machine 1</span></p>
<h3 id="Parallel-Running-Time-on-Machine-2-Windows"><a href="#Parallel-Running-Time-on-Machine-2-Windows" class="headerlink" title="Parallel Running Time on Machine 2: Windows"></a>Parallel Running Time on Machine 2: Windows</h3><p>Machine 2 is a Windows laptop with 2 intel i7 processors and 4 threads. From Fig. 6 we can see Dijkstra’s running time keeps decreasing as more threads are used. The 4 thread mode shows good performance, which is better than the macbook pro.<br><img src="/img/Dip_threads.png" alt="Figure 6. Parallel Running Time on Machine 2"><span class="image_caption">Figure 6. Parallel Running Time on Machine 2</span></p>
<h3 id="Parallel-Running-Time-on-Machine-3-Ubuntu"><a href="#Parallel-Running-Time-on-Machine-3-Ubuntu" class="headerlink" title="Parallel Running Time on Machine 3: Ubuntu"></a>Parallel Running Time on Machine 3: Ubuntu</h3><p>Machine 3 has 4 physical cores and 8 total threads. The results in Fig. 7 show good speed up going from 1 to 4 threads. However, the 8 threads mode is not very helpful.<br><img src="/img/Apoorva_threads.png" alt="Figure 7. Parallel Running Time on Machine 3"><span class="image_caption">Figure 7. Parallel Running Time on Machine 3</span></p>
<h3 id="Parallel-Running-Time-on-Machine-4-Cent-OS"><a href="#Parallel-Running-Time-on-Machine-4-Cent-OS" class="headerlink" title="Parallel Running Time on Machine 4: Cent OS"></a>Parallel Running Time on Machine 4: Cent OS</h3><p>The personal laptops have limited number of CPU cores. But for Farber clusters, it has 20 cores each node, making it good to explore the speed up with more cores. So on machine 4 we focus on the speed up with more threads and just run the three algorithms on one graph with 1000 nodes. The results running with programs compiled with GCC are plotted in Fig. 8. The Dijkstra’s running time start to increase after 4 threads and goes to very high with 32 threads. Further investigation is needed to find out the reasons. The other two algorithms show decreasing running time up to 16 threads. The 32 threads’ running time start to increase as there’re only 20 physical cores.<br><img src="/img/Farber_threads.png" alt="Figure 8. Parallel Running Time on Machine 4"><span class="image_caption">Figure 8. Parallel Running Time on Machine 4</span></p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><h3 id="Amdahl’s-Law-and-Roofline-Model"><a href="#Amdahl’s-Law-and-Roofline-Model" class="headerlink" title="Amdahl’s Law and Roofline Model"></a>Amdahl’s Law and Roofline Model</h3><p>A general trend in all machines and all algorithms is that ast the number of node increases, the speed up effect becomes more prominent. The average change in running time from one to two threads for graphs with different nodes is shown in Fig. 9. The more nodes in the graph, the more prominence of scale up with more threads. This could be related to Amdahl’s law, which says taht performance improvement to be gained from using some faster mode of execution is limited by the fraction of the time the faster mode can be used. With more nodes in the graph, a larger fraction of the code can be parallelized.<br><img src="/img/Decrease with nodes.png" alt="Figure 9. Speed up with respect to graph size"><span class="image_caption">Figure 9. Speed up with respect to graph size</span></p>
<p>Roofline model tells us that the more arithmetic intensity an algorithm has, the more performance gain can be achieved using parallelization. Arithmetic intensity is the ratio of the total floating point operations to the total data movement. In our problem, the three algorithms has different arithmetic intensity, we would expect different speed up with the three algorithms. Fig. 10 shows the change in running time with different algorithms when going from 1 thread to 2 threads. Floyd-Warshall’s algorithm shows the largest decrease in running time, which is as expected as it has the largest arithmetic intensity with running time scales like $O(v^3)$. Our results correctly demonstrate the Roofline model.<br><img src="/img/Decrease with algorithm.png" alt="Figure 9 Speed up with respect to different algorithms"><span class="image_caption">Figure 9 Speed up with respect to different algorithms</span></p>
<h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><p>There are many applications for shortest path algorithm. To name a few:</p>
<ol>
<li>Shortest Path Algorithms are used in google maps to find the short path between the source and destination. We can also say it as it is used to find the direction between two physical locations. Google maps uses A* algorithm to do the same.</li>
<li>Shortest path algorithm is used in IP routing to find the Open shortest path first.</li>
<li>It is also used in telephone networks.</li>
<li>Used to find arbitrage opportunities in currency exchange problem.</li>
</ol>
<h3 id="Finding-Arbitrage-Opportunities-in-Currency-Exchange-Problem"><a href="#Finding-Arbitrage-Opportunities-in-Currency-Exchange-Problem" class="headerlink" title="Finding Arbitrage Opportunities in Currency Exchange Problem."></a>Finding Arbitrage Opportunities in Currency Exchange Problem.</h3><p>For this problem we use Bellman Ford Algorithm. Bellman ford is used because the graph can have negative edges and this algorithm can be used to find if the graph has any negative cycles. For this problem we use negative log of the exchange rates to change as the weights of the edges. This done to change the problem from maximization problem to minimization problem. The Table 4. gives details about the exchange rates between the 6 different currencies. If the graph has any negative cycles indicates that this currency exchange has arbitrage opportunities. We can use backtracking to find between which currency exchange pair this arbitrage opportunity exists. Arbitrage opportunity results from a pricing discrepancy among the different currencies in the foreign exchange market.<br><img src="/img/Currency.png" alt=""></p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>In conclusion, parallelization definitely reduces the time cost compared to the sequential run time of the three algorithms. Doing this project brought into light that the speedup of thread-level parallelization depends on the machine as well as the operating system. The results produced demonstrates Amdahl’s law and the Roofline model. Not only were the benefits of parallelization exposed but to see the algorithms being applied to a useful application like currency exchange was quite interesting. </p>
<h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>Further investigation can be done on the parallelization of Dijkstra’s algorithm and seek to improve the runtime for more number of threads. It would also be fascinating to apply these effective algorithms for more applications like the GPS system, electronic design, telephone networks, etc. The shortest path problem is certainly worth exploring due to its countless implementations. </p>
<p>All the code and data sets can be found in the GitHub repository as listed in [9].</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1]. Popa, B., &amp; Popescu, D. (2016, May). Analysis of algorithms for shortest path problem in parallel. In Carpathian Control Conference (ICCC), 2016 17th International (pp. 613-617). IEEE.</p>
<p>[2]. Crauser, A., Mehlhorn, K., Meyer, U., &amp; Sanders, P. (1998, August). A parallelization of Dijkstra’s shortest path algorithm. In International Symposium on Mathematical Foundations of Computer Science (pp. 722-731). Springer, Berlin, Heidelberg.</p>
<p>[3]. Bellman-Ford algorithm in Parallel and Serial - GitHub link<br><a href="https://github.com/sunnlo/BellmanFord" target="_blank" rel="noopener">https://github.com/sunnlo/BellmanFord</a></p>
<p>[4]. Dijkstra’s Shortest Path Algorithm<br><a href="https://people.sc.fsu.edu/~jburkardt/cpp_src/dijkstra/dijkstra.cpp" target="_blank" rel="noopener">https://people.sc.fsu.edu/~jburkardt/cpp_src/dijkstra/dijkstra.cpp</a> </p>
<p>[5]. Floyd, R. W. (1962). Algorithm 97: shortest path. Communications of the ACM, 5(6), 345.</p>
<p>[6]. Floyd Warshall Algorithm<br><a href="https://engineering.purdue.edu/~eigenman/ECE563/ProjectPresentations/ParallelAll-PointsShortestPaths.pdf" target="_blank" rel="noopener">https://engineering.purdue.edu/~eigenman/ECE563/ProjectPresentations/ParallelAll-PointsShortestPaths.pdf</a> </p>
<p>[7]. Bellman-Ford Algorithm Tutorial<br><a href="https://www.programiz.com/dsa/bellman-ford-algorithm" target="_blank" rel="noopener">https://www.programiz.com/dsa/bellman-ford-algorithm</a> </p>
<p>[8]. Shortest Path Algorithms Tutorial <a href="https://www.hackerearth.com/practice/algorithms/graphs/shortest-path-algorithms/tutorial" target="_blank" rel="noopener">https://www.hackerearth.com/practice/algorithms/graphs/shortest-path-algorithms/tutorial</a></p>
<p>[9]. <a href="https://github.com/Zhiqiang-UD/CISC662" target="_blank" rel="noopener">https://github.com/Zhiqiang-UD/CISC662</a></p>

                
            </div>
            <div class="continue">
            <a href="/2018/12/12/CISC_662_Project_Report/">
            Continue            <i class="fa fa-angle-right" aria-hidden="true"></i>
            </a>
            </div>
        </article>
        
        <article class="post">
            <h1>
                <a class="title" href="/2018/07/15/Scrapy1/"> 
                    Scrapy Tutorial 1: overview 
                </a>
            </h1>
                        <div class="meta">
                <a class="date"> 
                    <i class="fa fa-calendar" aria-hidden="true"></i>                    
                    2018-07-15   
                </a>
                
                
                
                    
            </div>
            <div class="content">
                
                <h2 id="About-Scrapy"><a href="#About-Scrapy" class="headerlink" title="About Scrapy"></a>About Scrapy</h2><blockquote>
<p><strong>Scrapy</strong>  is a free and  open source <a href="https://en.wikipedia.org/wiki/Web_crawler" target="_blank" rel="noopener">web crawling</a>   framework , written in Python. Originally designed for web scraping, it can also be used to extract data using API.  or as a general purpose web crawler. It is currently maintained by  <a href="https://en.wikipedia.org/w/index.php?title=Scrapinghub&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">Scrapinghub Ltd.</a> , a web scraping development and services company.  </p>
</blockquote>
<h2 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h2><h3 id="Data-Flow"><a href="#Data-Flow" class="headerlink" title="Data Flow"></a>Data Flow</h3><p>The following diagram shows an overview of the Scrapy architecture with its components and and outline of data flow (red arrows).<br><img src="https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png" alt="architecture"><span class="image_caption">architecture</span><br>The data flow is controlled by the execution engine and goes like this (as indicated by the red arrow):</p>
<ol>
<li>The <strong>Engine</strong> gets the initial <em>Requests</em> to crawl from the <strong>Spiders</strong>.</li>
<li>The <strong>Engine</strong> schedules the <em>Requests</em> in the <strong>Scheduler</strong> and ask for the next <em>Requests</em> to crawl.</li>
<li>The <strong>Scheduler</strong> sends back the next <em>Requests</em> to the <strong>Engine</strong>.</li>
<li>The <strong>Engine</strong> send the <em>Requests</em> to the <strong>Downloader</strong> through the <strong>Downloader Middlewares</strong> (see process_request()).</li>
<li>Once the <strong>Downloader</strong> finishes the downloading it generates a <em>Response</em> and sends it back to <strong>Engine</strong> through the <strong>Downloader Middlewares</strong> (see process_response()).</li>
<li>The <strong>Engine</strong> sends the received <em>Response</em> to the <strong>Spiders</strong> for processing through the <strong>Spider Middleware</strong> (see process_spider_input()).</li>
<li>The <strong>Spiders</strong> processes the <em>Response</em> and returns the scraped <em>Items</em> and new <em>Requests</em> (to follow) to the <strong>Engine</strong> through the <strong>Spider Middleware</strong> (see process_spider_output()).</li>
<li>The <strong>Engine</strong> sends the scraped <em>Items</em> to <strong>Item Pipelines</strong>, then send the processed <em>Requests</em> to the <strong>Scheduler</strong> and ask for the next possible <em>Requests</em> to crawl.</li>
<li>The process repeats (from step 1) until there are no more requests from the <strong>Spiders</strong>.</li>
</ol>
<h3 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h3><h4 id="Scrapy-Engine"><a href="#Scrapy-Engine" class="headerlink" title="Scrapy Engine"></a>Scrapy Engine</h4><blockquote>
<p>The engine controls the data flow between all components and triggers events when certain action occurs. See <a href="#Data-Flow">Data Flow</a> for more details.</p>
</blockquote>
<h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><blockquote>
<p>The Scheduler receives the request from the engine and enqueues them for feeding them back to engine later when requested.</p>
</blockquote>
<h4 id="Downloader"><a href="#Downloader" class="headerlink" title="Downloader"></a>Downloader</h4><blockquote>
<p>The Downloader is responsible for fetching web pages from the Internet and feeding them back to the engine.</p>
</blockquote>
<h4 id="Spiders"><a href="#Spiders" class="headerlink" title="Spiders"></a>Spiders</h4><blockquote>
<p>Spiders are custom classes written by the user to parse responses and extract scraped items from them or additional requests to follow. Each spider is used for one (or a series of) specific webpage.</p>
</blockquote>
<h4 id="Item-Pipelines"><a href="#Item-Pipelines" class="headerlink" title="Item Pipelines"></a>Item Pipelines</h4><blockquote>
<p>The Item Pipelines is responsible for processing the extracted items from the spiders. Typical tasks include cleansing, validation and persistence (like stoing the item in a database)</p>
</blockquote>
<h4 id="Downloader-Middleware"><a href="#Downloader-Middleware" class="headerlink" title="Downloader Middleware"></a>Downloader Middleware</h4><blockquote>
<p>Downloader Middleware is a specific hook between the Engine the the Downloader and processes requests when pass from the Engine to the Downloader and responses that pass from Downloader to the Engine. It provides a simple mechanism to extend Scrapy by inserting user defined code, like automatic replace user-agent, IP, etc.</p>
</blockquote>
<h4 id="Spider-Middleware"><a href="#Spider-Middleware" class="headerlink" title="Spider Middleware"></a>Spider Middleware</h4><blockquote>
<p>Spider Middleware is a specific hook between the Engine and the Spider and processes spider input (response) and output (items and request). It also provides a simple mechanism to extend Scrapy functions by using user-defined code.</p>
</blockquote>
<h2 id="Process-to-Create-a-Scrapy-Project"><a href="#Process-to-Create-a-Scrapy-Project" class="headerlink" title="Process to Create a Scrapy Project"></a>Process to Create a Scrapy Project</h2><h3 id="Create-Project"><a href="#Create-Project" class="headerlink" title="Create Project"></a>Create Project</h3><p>First you need to create a Scrapy project. I’ll use the England Premier League website as an example. Run the following command:</p>
<pre><code class="lang-Python">scrapy startproject EPLspider
</code></pre>
<p>The EPLspider directory with the following content will be created:</p>
<pre><code>EPLspider/
├── EPLspider
│   ├── __init__.py
│   ├── __pycache__
│   ├── items.py
│   ├── middlewares.py
│   ├── pipelines.py
│   ├── settings.py
│   └── spiders
│       ├── __init__.py
│       └── __pycache__
└── scrapy.cfg
</code></pre><p>The content of each file:</p>
<ul>
<li>EPLspider/: Python module of the project, in which code will be added.</li>
<li>EPLspider/items.py: item file of the project.</li>
<li>EPLspider/middlewares.py: middlewares file of the project.</li>
<li>EPLspider/pipelines: pipelines file of the project.</li>
<li>EPLspider/settings: settings file of the project.</li>
<li>EPLspider/spiders/: directory with spider code.</li>
<li>scrapy.cfg: configuration file of the Project.</li>
</ul>
<h3 id="Start-with-the-First-Spider"><a href="#Start-with-the-First-Spider" class="headerlink" title="Start with the First Spider"></a>Start with the First Spider</h3><blockquote>
<p>Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass <em>scrapy.Spider</em> and define the initial request to make, optionally how to deal with links in the pages, and how to parse the downloaded page content to extract data.</p>
</blockquote>
<p>This is our first Spider, <code>EPL_spider.py</code>, saved in the directory <code>EPLspider/spiders/</code>.</p>
<pre><code>from scrapy.spiders import Spider

class EPLspider(Spider):
    name = &#39;premierLeague&#39;
    start_urls = [&#39;https://www.premierleague.com/clubs&#39;]

    def parse(self, response):
        club_url_list = response.css(&#39;ul[class=&quot;block-list-5 block-list-3-m block-list-2-s block-list-2-xs block-list-padding dataContainer&quot;] ::attr(href)&#39;).extract()
        club_name = response.css(&#39;h4[class=&quot;clubName&quot;]::text&#39;).extract()
        club_stadium = response.css(&#39;div[class=&quot;stadiumName&quot;]::text&#39;).extract()
        for i,j in zip(club_name, club_stadium):
            print(i, j)
</code></pre><h3 id="Run-the-Spider"><a href="#Run-the-Spider" class="headerlink" title="Run the Spider"></a>Run the Spider</h3><p>Run the following command in the project folder:</p>
<pre><code>scrapy crawl premierLeague
</code></pre><p>The club name and stadium of all clubs from the England Premier League will be printed out.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this tutorial we show the overall architecture of Scrapy and show its basics with a demo. In the next tutorial, we’ll extend this simple spider program to get more detailed information about the England Premier League, i.e. clubs, players, managers, and match information, <em>etc</em>.</p>

                
            </div>
            <div class="continue">
            <a href="/2018/07/15/Scrapy1/">
            Continue            <i class="fa fa-angle-right" aria-hidden="true"></i>
            </a>
            </div>
        </article>
        
        <article class="post">
            <h1>
                <a class="title" href="/2018/07/13/hello-world_new/"> 
                    Hello World 
                </a>
            </h1>
                        <div class="meta">
                <a class="date"> 
                    <i class="fa fa-calendar" aria-hidden="true"></i>                    
                    2018-07-13   
                </a>
                
                
                
                    
            </div>
            <div class="content">
                
                <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;
</code></pre>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server
</code></pre>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate
</code></pre>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy
</code></pre>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

                
            </div>
            <div class="continue">
            <a href="/2018/07/13/hello-world_new/">
            Continue            <i class="fa fa-angle-right" aria-hidden="true"></i>
            </a>
            </div>
        </article>
        
</section>

    </main>
    <a class="not-found">not found!</a>
    <div class="search-items">
    </div>
    <a href="#header" id="top" style="display:none">
        <i class="fa fa-sort-asc fa-2x"></i>
    </a>
    <footer class="footer">
    <div class="footer-copyright">©️2017
    <a href="//github.com/Vevlins/toki" class="link" target="_blank">Toki</a>  by Vevlins
    </div>
</footer>

    <script src="/js/jquery.js"></script>
    <script src="/js/toki.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
