<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zhiqiang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-12-13T03:27:28.175Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Zhiqiang Zhang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Parallelizing and Analysis of Shortest Path Algorithms </title>
    <link href="http://yoursite.com/2018/12/12/CISC_662_Project_Report/"/>
    <id>http://yoursite.com/2018/12/12/CISC_662_Project_Report/</id>
    <published>2018-12-13T03:27:28.174Z</published>
    <updated>2018-12-13T03:27:28.175Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A central problem in graph theory is the shortest path problem which is to find the shortest path between two nodes (vertices) in a graph so that the sum of the weight of its constituent edges is minimum. There are four variations of the shortest path problem namely, single-source shortest path (SSSP), breadth-first search (BFS), all-pairs shortest path (APSP), and single-source widest path (SSWP). Many algorithms are in use today which solve this problem. Some of the most popular algorithms are Dijkstra’s algorithm, Bellman-Ford algorithm, A* search algorithm, and Floyd-Warshall algorithm. These algorithms can be applied to undirected as well as directed graphs. Interesting research has been done on this problem due to its many useful real-world applications like road networks, community detection, currency exchange, logistics, electronic design, etc. In this project, we focused on the parallelizing of three algorithms - Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm and analyze the speedups against sequential implementations.</p><h2 id="Literature-Review"><a href="#Literature-Review" class="headerlink" title="Literature Review"></a>Literature Review</h2><p>Parallelizing the classical shortest path algorithms can be done in several ways. Some of the ways have been discussed by B. Popa <em>et al.</em> [1] Another approach to parallelize Dijkstra’s algorithm has been discussed by A. Crauser <em>et al.</em> [2]<br>To discuss parallelizing the classical algorithms, we discuss the main idea of the three algorithms we used in our experiment, namely: Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm in the following.<br>Dijkstra’s algorithm finds the shortest path from a single source and at each step, it finds the minimal distant node, tries to find the shortest paths to other nodes using the recently found shortest node. This algorithm does not consider negative weights and thus cannot be used in a graph with negative weights.<br>Bellman-Ford algorithm also finds the shortest path from a single source. However, at each step in contrary to Dijkstra’s algorithm, this algorithm updates each node’s distance from the source by the observing the edges. Bellman-Ford algorithm can detect negative weight cycles in a graph and also can be used in a graph with negative weights.<br>Floyd-Warshall algorithm finds all pair shortest path and does that by observing each node at a time and update paired shortest distances if the node can contribute. This algorithm also can be used in a graph with negative weights.<br>The pseudocodes of Dijkstra’s algorithm, Bellman-Ford algorithm and Floyd-Warshall algorithm are given in Figure 2.1 below.<br>| <img src="/img/Fig2.1.png" alt="space-1.jpg"> |<br>|:–:|<br>| <em>Space</em> |</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;A central problem in graph the
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Scrapy Tutorial 1: overview</title>
    <link href="http://yoursite.com/2018/07/15/Scrapy1/"/>
    <id>http://yoursite.com/2018/07/15/Scrapy1/</id>
    <published>2018-07-15T21:02:27.662Z</published>
    <updated>2018-07-17T02:27:13.406Z</updated>
    
    <content type="html"><![CDATA[<h2 id="About-Scrapy"><a href="#About-Scrapy" class="headerlink" title="About Scrapy"></a>About Scrapy</h2><blockquote><p><strong>Scrapy</strong>  is a free and  open source <a href="https://en.wikipedia.org/wiki/Web_crawler" target="_blank" rel="noopener">web crawling</a>   framework , written in Python. Originally designed for web scraping, it can also be used to extract data using API.  or as a general purpose web crawler. It is currently maintained by  <a href="https://en.wikipedia.org/w/index.php?title=Scrapinghub&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">Scrapinghub Ltd.</a> , a web scraping development and services company.  </p></blockquote><h2 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h2><h3 id="Data-Flow"><a href="#Data-Flow" class="headerlink" title="Data Flow"></a>Data Flow</h3><p>The following diagram shows an overview of the Scrapy architecture with its components and and outline of data flow (red arrows).<br><img src="https://doc.scrapy.org/en/latest/_images/scrapy_architecture_02.png" alt="architecture"><br>The data flow is controlled by the execution engine and goes like this (as indicated by the red arrow):</p><ol><li>The <strong>Engine</strong> gets the initial <em>Requests</em> to crawl from the <strong>Spiders</strong>.</li><li>The <strong>Engine</strong> schedules the <em>Requests</em> in the <strong>Scheduler</strong> and ask for the next <em>Requests</em> to crawl.</li><li>The <strong>Scheduler</strong> sends back the next <em>Requests</em> to the <strong>Engine</strong>.</li><li>The <strong>Engine</strong> send the <em>Requests</em> to the <strong>Downloader</strong> through the <strong>Downloader Middlewares</strong> (see process_request()).</li><li>Once the <strong>Downloader</strong> finishes the downloading it generates a <em>Response</em> and sends it back to <strong>Engine</strong> through the <strong>Downloader Middlewares</strong> (see process_response()).</li><li>The <strong>Engine</strong> sends the received <em>Response</em> to the <strong>Spiders</strong> for processing through the <strong>Spider Middleware</strong> (see process_spider_input()).</li><li>The <strong>Spiders</strong> processes the <em>Response</em> and returns the scraped <em>Items</em> and new <em>Requests</em> (to follow) to the <strong>Engine</strong> through the <strong>Spider Middleware</strong> (see process_spider_output()).</li><li>The <strong>Engine</strong> sends the scraped <em>Items</em> to <strong>Item Pipelines</strong>, then send the processed <em>Requests</em> to the <strong>Scheduler</strong> and ask for the next possible <em>Requests</em> to crawl.</li><li>The process repeats (from step 1) until there are no more requests from the <strong>Spiders</strong>.</li></ol><h3 id="Components"><a href="#Components" class="headerlink" title="Components"></a>Components</h3><h4 id="Scrapy-Engine"><a href="#Scrapy-Engine" class="headerlink" title="Scrapy Engine"></a>Scrapy Engine</h4><blockquote><p>The engine controls the data flow between all components and triggers events when certain action occurs. See <a href="#Data-Flow">Data Flow</a> for more details.</p></blockquote><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><blockquote><p>The Scheduler receives the request from the engine and enqueues them for feeding them back to engine later when requested.</p></blockquote><h4 id="Downloader"><a href="#Downloader" class="headerlink" title="Downloader"></a>Downloader</h4><blockquote><p>The Downloader is responsible for fetching web pages from the Internet and feeding them back to the engine.</p></blockquote><h4 id="Spiders"><a href="#Spiders" class="headerlink" title="Spiders"></a>Spiders</h4><blockquote><p>Spiders are custom classes written by the user to parse responses and extract scraped items from them or additional requests to follow. Each spider is used for one (or a series of) specific webpage.</p></blockquote><h4 id="Item-Pipelines"><a href="#Item-Pipelines" class="headerlink" title="Item Pipelines"></a>Item Pipelines</h4><blockquote><p>The Item Pipelines is responsible for processing the extracted items from the spiders. Typical tasks include cleansing, validation and persistence (like stoing the item in a database)</p></blockquote><h4 id="Downloader-Middleware"><a href="#Downloader-Middleware" class="headerlink" title="Downloader Middleware"></a>Downloader Middleware</h4><blockquote><p>Downloader Middleware is a specific hook between the Engine the the Downloader and processes requests when pass from the Engine to the Downloader and responses that pass from Downloader to the Engine. It provides a simple mechanism to extend Scrapy by inserting user defined code, like automatic replace user-agent, IP, etc.</p></blockquote><h4 id="Spider-Middleware"><a href="#Spider-Middleware" class="headerlink" title="Spider Middleware"></a>Spider Middleware</h4><blockquote><p>Spider Middleware is a specific hook between the Engine and the Spider and processes spider input (response) and output (items and request). It also provides a simple mechanism to extend Scrapy functions by using user-defined code.</p></blockquote><h2 id="Process-to-Create-a-Scrapy-Project"><a href="#Process-to-Create-a-Scrapy-Project" class="headerlink" title="Process to Create a Scrapy Project"></a>Process to Create a Scrapy Project</h2><h3 id="Create-Project"><a href="#Create-Project" class="headerlink" title="Create Project"></a>Create Project</h3><p>First you need to create a Scrapy project. I’ll use the England Premier League website as an example. Run the following command:</p><pre><code class="Python">scrapy startproject EPLspider</code></pre><p>The EPLspider directory with the following content will be created:</p><pre><code>EPLspider/├── EPLspider│   ├── __init__.py│   ├── __pycache__│   ├── items.py│   ├── middlewares.py│   ├── pipelines.py│   ├── settings.py│   └── spiders│       ├── __init__.py│       └── __pycache__└── scrapy.cfg</code></pre><p>The content of each file:</p><ul><li>EPLspider/: Python module of the project, in which code will be added.</li><li>EPLspider/items.py: item file of the project.</li><li>EPLspider/middlewares.py: middlewares file of the project.</li><li>EPLspider/pipelines: pipelines file of the project.</li><li>EPLspider/settings: settings file of the project.</li><li>EPLspider/spiders/: directory with spider code.</li><li>scrapy.cfg: configuration file of the Project.</li></ul><h3 id="Start-with-the-First-Spider"><a href="#Start-with-the-First-Spider" class="headerlink" title="Start with the First Spider"></a>Start with the First Spider</h3><blockquote><p>Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass <em>scrapy.Spider</em> and define the initial request to make, optionally how to deal with links in the pages, and how to parse the downloaded page content to extract data.</p></blockquote><p>This is our first Spider, <code>EPL_spider.py</code>, saved in the directory <code>EPLspider/spiders/</code>.</p><pre><code>from scrapy.spiders import Spiderclass EPLspider(Spider):    name = &#39;premierLeague&#39;    start_urls = [&#39;https://www.premierleague.com/clubs&#39;]    def parse(self, response):        club_url_list = response.css(&#39;ul[class=&quot;block-list-5 block-list-3-m block-list-2-s block-list-2-xs block-list-padding dataContainer&quot;] ::attr(href)&#39;).extract()        club_name = response.css(&#39;h4[class=&quot;clubName&quot;]::text&#39;).extract()        club_stadium = response.css(&#39;div[class=&quot;stadiumName&quot;]::text&#39;).extract()        for i,j in zip(club_name, club_stadium):            print(i, j)</code></pre><h3 id="Run-the-Spider"><a href="#Run-the-Spider" class="headerlink" title="Run the Spider"></a>Run the Spider</h3><p>Run the following command in the project folder:</p><pre><code>scrapy crawl premierLeague</code></pre><p>The club name and stadium of all clubs from the England Premier League will be printed out.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In this tutorial we show the overall architecture of Scrapy and show its basics with a demo. In the next tutorial, we’ll extend this simple spider program to get more detailed information about the England Premier League, i.e. clubs, players, managers, and match information, <em>etc</em>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;About-Scrapy&quot;&gt;&lt;a href=&quot;#About-Scrapy&quot; class=&quot;headerlink&quot; title=&quot;About Scrapy&quot;&gt;&lt;/a&gt;About Scrapy&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Scrapy&lt;/s
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/07/13/hello-world_new/"/>
    <id>http://yoursite.com/2018/07/13/hello-world_new/</id>
    <published>2018-07-14T03:49:54.288Z</published>
    <updated>2018-07-14T03:49:54.288Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
